# Automation bias: Presentation Notes 
One of the major ethical issues in software development today is how humans interact with AI, especially when we trust it too much. 
This is called automation bias, and it happens when people assume AI is always correct - even when it might be wrong.

In software development, automation bias often appears when programmers rely heavily on tools like Github Copilot or ChatGPT. 
These tools are designed to make coding faster and reduce repetitive tasks, but blindly trusting them can lead to ethical problems. 
For example, if a developer accepts AI suggestions without checking them, bugs or security flaws can slip into the software. 
This raises an important question: who would be responsible when something goes wrong? 

Automation bias also affects reliability. Over-relying on AI reduces critical thinking and can introduce errors that impact the 
software performance. A 2025 review of 90 studies on AI-assisted coding found that developers often assume AI suggestions are correct. 
Checking these suggestions carefully can take up to half their coding time, which means many developers - especially early
developers - may skip or rush verification.

So, the key takeaway is that automation bias reminds us we can't just take AI's suggestions at face value. We need to stay in control, 
double-check what it tells us and think criticaally about our decisions. 

## Presentation Slide Layout 
- **Automation bias** = over-trusting AI, assuming it's always correct 
- Happens when developers rely too heavily on tools like Github Copilot or ChatGPT

**Ethical Concerns**
- Bugs, security flaws and mistakes
- Responsibilty: Who is accountable when errors occur?
- Reliability: Errors can affect software performance

**Key Takeaway:**
  - Humans must stay in control, check AI outputs and think critically.
  - Use AI responsibly. 

## Reference 
Sergeyuk, A., Zakharov, I., Koshchenko, E., & Izadi, M. (2025). Humanâ€“AI experience in integrated development environments: A systematic literature review. arXiv. https://arxiv.org/abs/2503.06195
